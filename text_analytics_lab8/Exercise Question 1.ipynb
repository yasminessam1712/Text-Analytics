{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5ad50a-75c9-4587-8d53-172bbf8c4b2e",
   "metadata": {},
   "source": [
    "Text Clustering Using TF-IDF Vectorizer with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4759a8bf-390b-4842-a7f7-4a69692d1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfcb40c-5fc5-4923-aa77-97d38390bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and apply stemming\n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Return preprocessed text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535c464e-8bb3-444d-9f47-f7213cc89749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the Documents\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba4319d-614f-4cd1-9195-0878e080b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " book\n",
      " like\n",
      " read\n",
      " movi\n",
      " watch\n",
      " camp\n",
      " concert\n",
      " enjoy\n",
      " footbal\n",
      " game\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " play\n",
      " footbal\n",
      " weekend\n",
      " enjoy\n",
      " camp\n",
      " mountain\n",
      " hike\n",
      " sport\n",
      " music\n",
      "\n",
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "# Step 3: Vectorize the Dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)\n",
    "\n",
    "# Step 4: Perform Clustering\n",
    "k = 2  # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Step 5: Print Top Terms Per Cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "\n",
    "# Step 6: Evaluate Results\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3070cc3-2468-4b19-b94d-b1bc16b45d60",
   "metadata": {},
   "source": [
    "Text Clustering Using Word2Vec Vectorizer with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd85d37c-d834-4d94-91e5-0722204f9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n",
      "Purity: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and apply stemming\n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Return preprocessed text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Step 2: Create the Documents\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "# Step 3: Train Word2Vec Model\n",
    "tokenized_dataset = [doc.split() for doc in preprocessed_dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100,\n",
    "                           window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 4: Create Document Embeddings\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in \n",
    "                       word2vec_model.wv], axis=0) for doc in preprocessed_dataset])\n",
    "\n",
    "# Step 5: Perform Clustering\n",
    "k = 2  # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Step 6: Evaluate Results\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df7102-913b-40b6-bb51-4d0f38574cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
